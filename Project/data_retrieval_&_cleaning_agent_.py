# -*- coding: utf-8 -*-
"""Data Retrieval & Cleaning Agent .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/188dbvLdyLOsl3Ki6TqwhHIge3H5aClun
"""

!pip install langgraph langchain-openai langchain-core feedparser -q

from typing import Dict, List, Optional, Any, TypedDict
from dataclasses import dataclass
from enum import Enum
from datetime import datetime, timedelta, timezone
import logging, json, asyncio, re, os, time, html
import feedparser
import hashlib
import math

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage

# Logging
logger = logging.getLogger("ci_agents")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
logger.addHandler(handler)

PRICE_PER_1K = {
    "gpt-4o-mini":   {"input": 0.150, "output": 0.600},
    "gpt-4o":        {"input": 2.500, "output": 5.000},
    "gpt-4.1-mini":  {"input": 0.300, "output": 1.200},
    "gpt-4":         {"input": 3.000, "output": 6.000},
}
def _estimate_cost(model: str, usage: dict) -> float:
    pr = PRICE_PER_1K.get(model, None)
    if not pr:
        return 0.0
    in_t  = usage.get("input_tokens", 0) or usage.get("prompt_tokens", 0) or 0
    out_t = usage.get("output_tokens", 0) or usage.get("completion_tokens", 0) or 0
    return (in_t/1000.0)*pr["input"] + (out_t/1000.0)*pr["output"]

OPENAI_API_KEY = os.environ.get("KEY")
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY is not set in environment. Set it before running LLM calls.")

"""# Models and Agent State"""

class CompanySize(Enum):
    SMALL = "small"
    MEDIUM = "medium"
    LARGE = "large"

class ContentType(Enum):
    PRODUCT_LAUNCH = "product_launch"
    REVIEW = "review"
    PRICING = "pricing"
    RUMOR = "rumor"
    SOFTWARE_UPDATE = "software_update"
    PARTNERSHIP = "partnership"
    AI_FEATURE = "ai_feature"
    SECURITY = "security"
    GENERAL_NEWS = "general_news"

class SentimentScore(Enum):
    VERY_POSITIVE = 2
    POSITIVE = 1
    NEUTRAL = 0
    NEGATIVE = -1
    VERY_NEGATIVE = -2

class ThreatLevel(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

@dataclass
class CompetitorProfile:
    name: str
    size: CompanySize
    market_share: float
    is_direct_competitor: bool
    recent_growth: str
    os_ecosystem: str
    focus_regions: List[str]
    ai_capabilities: List[str]
    key_products: List[str]

# Define the competitor profiles with enhanced data
COMPETITOR_PROFILES = {
    "Apple": CompetitorProfile(
        "Apple", CompanySize.LARGE, market_share=0.20,
        is_direct_competitor=False, recent_growth="growing",
        os_ecosystem="ios", focus_regions=["US","EU","KSA","UAE","IN"],
        ai_capabilities=["Siri", "Neural Engine", "Core ML", "Face ID", "AI Camera"],
        key_products=["iPhone", "iPad", "Mac", "Apple Watch", "AirPods"]
    ),
    "Samsung": CompetitorProfile(
        "Samsung", CompanySize.LARGE, market_share=0.21,
        is_direct_competitor=True, recent_growth="growing",
        os_ecosystem="android", focus_regions=["EU","KSA","UAE","IN","US"],
        ai_capabilities=["Bixby", "Galaxy AI", "SmartThings", "Knox", "AI Photography"],
        key_products=["Galaxy S", "Galaxy Z", "Galaxy A", "Galaxy Watch", "Buds"]
    ),
    "Xiaomi": CompetitorProfile(
        "Xiaomi", CompanySize.LARGE, market_share=0.13,
        is_direct_competitor=True, recent_growth="growing",
        os_ecosystem="android", focus_regions=["EU","IN","KSA","UAE","EG"],
        ai_capabilities=["XiaoAI", "HyperOS AI", "AI Camera", "Smart Scene"],
        key_products=["Mi series", "Redmi", "Poco", "Black Shark", "Pad"]
    ),
    "OPPO": CompetitorProfile(
        "OPPO", CompanySize.LARGE, market_share=0.08,
        is_direct_competitor=True, recent_growth="stable",
        os_ecosystem="android", focus_regions=["IN","KSA","UAE","EG"],
        ai_capabilities=["Breeno", "AI Camera", "ColorOS AI", "AI Optimization"],
        key_products=["Find X", "Reno", "A series", "K series", "Pad"]
    ),
    "vivo": CompetitorProfile(
        "vivo", CompanySize.LARGE, market_share=0.08,
        is_direct_competitor=True, recent_growth="stable",
        os_ecosystem="android", focus_regions=["IN"],
        ai_capabilities=["Jovi", "AI Camera", "Funtouch OS AI", "AI Assistant"],
        key_products=["X series", "V series", "Y series", "iQOO", "Pad"]
    ),
    "Huawei": CompetitorProfile(
        "Huawei", CompanySize.LARGE, market_share=0.07,
        is_direct_competitor=True, recent_growth="growing",
        os_ecosystem="mixed", focus_regions=["KSA","UAE","CN"],
        ai_capabilities=["Celia", "HiAI", "HarmonyOS AI", "AI Imaging"],
        key_products=["P series", "Mate series", "Nova", "Enjoy", "Tablet"]
    )
}

# Define the state that will be passed between nodes
class AgentState(TypedDict):
    """The state of our agent workflow."""
    messages: List[Dict[str, Any]]
    competitor_profiles: Dict[str, CompetitorProfile]
    target_regions: List[str]
    raw_articles: List[Dict[str, Any]]
    cleaned_articles: List[Dict[str, Any]]
    current_step: str
    error: str
    search_config: Dict[str, Any]

"""# Search Agent"""

class SearchAgent:
    """Enhanced agent responsible for fetching raw data about mobile phone companies"""

    def __init__(self):
        self.base_url = "https://news.google.com/rss/search?q={query}&hl={lang}&gl={country}&ceid={country}:en"
        self.request_delay = 0.7  # Slightly longer delay to be more polite
        self.max_articles_per_query = 30  # Limit results per query

    def __call__(self, state: AgentState) -> AgentState:
        """Execute search for multiple mobile companies across target regions"""
        print("🔍 Search Agent: Starting data collection for mobile companies...")

        competitor_profiles = state["competitor_profiles"]
        target_regions = state["target_regions"]
        search_config = state.get("search_config", {})
        all_articles = []

        # Use config values if available, otherwise defaults
        max_articles = search_config.get("max_articles_per_company", self.max_articles_per_query)
        timeframe_days = search_config.get("search_timeframe_days", 7)

        for company_name, profile in competitor_profiles.items():
            # Search in the company's focus regions that also match our target regions
            search_regions = [region for region in profile.focus_regions if region in target_regions]
            if not search_regions:
                search_regions = target_regions  # Fallback to all target regions

            for region in search_regions:
                print(f"   Searching for {company_name} in {region}...")
                articles = self._fetch_news(company_name, region, profile, max_articles, timeframe_days)
                all_articles.extend(articles)
                time.sleep(self.request_delay)  # Be polite to avoid rate limiting

        print(f"✅ Search Agent: Found {len(all_articles)} raw articles")

        return {
            "raw_articles": all_articles,
            "current_step": "data_cleaning",
            "error": "",
            "search_config": search_config
        }

    def _fetch_news(self, company: str, country: str, profile: CompetitorProfile,
                   max_articles: int, timeframe_days: int) -> List[Dict]:
        """Fetch news for a specific mobile company with enhanced domain-specific queries"""
        # Enhanced mobile industry search terms with AI focus
        query_terms = [
            f'"{company}"',
            'smartphone',
            'mobile phone',
            'cellphone',
            'handset',
            'android phone',
            'ios device',
            'launch',
            'release',
            'new model',
            '5G phone',
            'camera phone',
            'battery life',
            'phone specs',
            'phone review',
            'hands-on',
            'unboxing',
            'phone deal',
            'phone offer',
            # AI-specific terms
            'AI features',
            'artificial intelligence',
            'machine learning',
            'neural processing',
            'smart features',
            'virtual assistant',
            'neural engine',
            'AI camera',
            'generative AI'
        ]

        # Add company-specific AI capabilities to search terms
        if company in COMPETITOR_PROFILES:
            for ai_capability in COMPETITOR_PROFILES[company].ai_capabilities:
                query_terms.append(f'"{ai_capability}"')

            # Add company-specific products to search terms
            for product in COMPETITOR_PROFILES[company].key_products[:3]:
                query_terms.append(f'"{product}"')

        # Enhanced exclusion terms to avoid financial/news articles
        exclude_terms = [
            '-gold', '-silver', '-stock', '-market', '-investment',
            '-finance', '-economy', '-currency', '-rate', '-fed',
            '-price', '-record', '-bonanza', '-reuters', '-bloomberg',
            '-cnbc', '-financial', '-earnings', '-dividend', '-profit',
            '-trading', '-exchange', '-revenue', '-ipo', '-quarterly',
            '-financial results', '-stock price', '-market cap'
        ]

        query = " OR ".join(query_terms) + " " + " ".join(exclude_terms)

        # Add timeframe restriction for recent articles only
        date_restriction = datetime.now() - timedelta(days=timeframe_days)
        query += f" after:{date_restriction.strftime('%Y-%m-%d')}"

        formatted_url = self.base_url.format(
            query=query.replace(' ', '%20'),
            lang="en",  # Always use English for consistency
            country=country.lower()
        )

        print(f"      Search URL: {formatted_url[:120]}...")  # Debug: show the URL

        try:
            feed = feedparser.parse(formatted_url)
            articles = []

            for entry in feed.entries[:max_articles]:  # Limit results
                # Enhanced filtering: Skip articles that are clearly not about mobile phones
                title = entry.title.lower()
                summary = entry.summary.lower() if hasattr(entry, 'summary') else ''

                # Skip financial/news articles with enhanced filtering
                financial_keywords = [
                    'gold', 'silver', 'stock', 'market', 'investment',
                    'finance', 'economy', 'currency', 'rate', 'fed',
                    'earnings', 'profit', 'dividend', 'revenue', 'ipo',
                    'trading', 'exchange', 'dow jones', 'nasdaq', 's&p',
                    'quarterly', 'financial results', 'stock price', 'market cap',
                    'shareholder', 'dividend', 'revenue', 'profit margin'
                ]
                if any(keyword in title or keyword in summary for keyword in financial_keywords):
                    continue  # Skip this article

                # Must contain mobile/tech/AI-related keywords
                mobile_keywords = [
                    'phone', 'smartphone', 'mobile', 'android', 'ios',
                    'samsung', 'apple', 'xiaomi', 'oppo', 'vivo', 'huawei',
                    'ai', 'artificial intelligence', 'machine learning',
                    'camera', 'battery', 'processor', 'chip', '5g', 'device',
                    'galaxy', 'iphone', 'xiaomi', 'oppo', 'vivo', 'huawei',
                    'launch', 'release', 'announce', 'new', 'model', 'series'
                ]

                if not any(keyword in title or keyword in summary for keyword in mobile_keywords):
                    continue  # Skip this article

                # Calculate a relevance score based on keyword matches
                relevance_score = self._calculate_relevance_score(title, summary, company, profile)

                # Skip articles with very low relevance
                if relevance_score < 2:
                    continue

                article_data = {
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.published,
                    'summary': entry.summary if hasattr(entry, 'summary') else '',
                    'company': company,
                    'region': country,
                    'timestamp': datetime.now().isoformat(),
                    'raw_text': f"{entry.title}. {entry.summary if hasattr(entry, 'summary') else ''}",
                    'company_size': profile.size.value,
                    'market_share': profile.market_share,
                    'is_direct_competitor': profile.is_direct_competitor,
                    'os_ecosystem': profile.os_ecosystem,
                    'relevance_score': relevance_score,
                    'source': self._extract_source(entry.link),
                    'id': self._generate_article_id(entry)  # Unique ID for each article
                }
                articles.append(article_data)

            print(f"      Found {len(articles)} relevant articles for {company} in {country}")
            return articles

        except Exception as e:
            print(f"Error fetching news for {company} in {country}: {str(e)}")
            return []

    def _calculate_relevance_score(self, title: str, summary: str, company: str,
                                 profile: CompetitorProfile) -> int:
        """Calculate a relevance score based on keyword matches"""
        score = 0
        text = f"{title} {summary}".lower()

        # Company name mentions
        if company.lower() in text:
            score += 3

        # Product mentions
        for product in profile.key_products:
            if product.lower() in text:
                score += 2

        # AI-related terms
        ai_terms = [
            'ai', 'artificial intelligence', 'machine learning', 'neural',
            'algorithm', 'deep learning', 'nlp', 'computer vision',
            'generative ai', 'neural engine', 'neural processing'
        ]
        for term in ai_terms:
            if term in text:
                score += 2

        # Mobile-specific terms
        mobile_terms = [
            'phone', 'smartphone', 'mobile', 'android', 'ios', '5g',
            'camera', 'battery', 'processor', 'chip', 'display', 'specs'
        ]
        for term in mobile_terms:
            if term in text:
                score += 1

        return score

    def _extract_source(self, url: str) -> str:
        """Extract the source domain from a URL"""
        match = re.search(r'://([^/]+)', url)
        if match:
            return match.group(1)
        return "unknown"

    def _generate_article_id(self, entry) -> str:
        """Generate a unique ID for an article based on its content"""
        content = f"{entry.title}{entry.link}{entry.published}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_sentiment_label(self, score: int) -> str:
      """Convert sentiment score to readable label"""
      if score == 2:
          return "Very Positive"
      elif score == 1:
          return "Positive"
      elif score == 0:
          return "Neutral"
      elif score == -1:
          return "Negative"
      elif score == -2:
          return "Very Negative"
      else:
          return "Unknown"


print("Testing Enhanced Search Agent with ALL Competitors...")
search_agent = SearchAgent()
test_state = {
    "messages": [],
    "competitor_profiles": COMPETITOR_PROFILES,  # All competitors
    "target_regions": ["US", "EU", "KSA", "UAE", "IN", "EG", "CN"],  # All regions
    "raw_articles": [],
    "cleaned_articles": [],
    "current_step": "search",
    "error": "",
    "search_config": {
        "search_timeframe_days": 7  # 1 week timeframe only
    }
}

# Test with actual web requests
print("🔍 Performing actual search for ALL mobile competitors...")
print(f"Competitors: {', '.join(COMPETITOR_PROFILES.keys())}")
print(f"Regions: {', '.join(test_state['target_regions'])}")
print(f"Timeframe: Last {test_state['search_config']['search_timeframe_days']} days")
print()

test_results = search_agent(test_state)
print(f"✅ Search completed. Found {len(test_results['raw_articles'])} raw articles")

# Display raw articles summary by company
print("\n📄 RAW ARTICLES SUMMARY BY COMPANY:")
print("=" * 80)
company_counts = {}
for article in test_results['raw_articles']:
    company = article['company']
    company_counts[company] = company_counts.get(company, 0) + 1

for company, count in sorted(company_counts.items(), key=lambda x: x[1], reverse=True):
    print(f"{company}: {count} articles")

# Display sample raw articles
print(f"\n📄 SAMPLE RAW ARTICLES (First 5):")
print("=" * 80)
for i, article in enumerate(test_results['raw_articles'][:5]):
    print(f"{i+1}. {article['title']}")
    print(f"   Source: {article.get('source', 'Unknown')}")
    print(f"   Company: {article['company']}, Region: {article['region']}")
    print(f"   Relevance Score: {article.get('relevance_score', 'N/A')}")
    print()

# test_results['raw_articles'][0]

"""# Cleaning Agent"""

class CleaningAgent:
    """AI-Powered agent for cleaning and enriching mobile industry data with an LLM."""

    def __init__(self):
        self.mobile_brands = set(COMPETITOR_PROFILES.keys())
        self.mobile_brands_lower = {brand.lower() for brand in self.mobile_brands}

        # Initialize LangChain LLM - ADD THIS
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",  # Use your preferred model
            temperature=0.1,
            api_key=OPENAI_API_KEY
        )

        # Create the prompt template - ADD THIS
        self.analysis_prompt_template = ChatPromptTemplate.from_messages([
            ("system", """You are a senior competitive intelligence analyst specializing in the mobile phone industry. Your task is to analyze a news article snippet and extract structured insights.

Return ONLY a valid JSON object with the following structure:
{{
  "content_type": "string", // Choose ONLY one: "product_launch", "review", "pricing", "rumor", "software_update", "ai_feature", "partnership", "security", "general_news"
  "sentiment_score": number, // Range from -2 (Very Negative) to 2 (Very Positive)
  "contains_ai_mentions": boolean, // True if the article discusses AI, ML, neural processing, generative AI, etc.
  "competitive_intelligence_value": boolean, // True if this contains info useful for competitive strategy
  "key_takeaways": string // A concise, 1-2 sentence summary of the MOST important competitive information.
}}

CRITICAL INSTRUCTIONS:
1. **Content Type:** Be specific. "ai_feature" trumps "product_launch" if the launch is primarily about AI.
2. **Sentiment:** Base the score on the tone towards the company's technology, not the writing style.
3. **AI Mentions:** Be precise. "AI camera" counts, vague "smart features" do not unless context is clear.
4. **Competitive Value:** Is this actionable intelligence?
5. **Key Takeaways:** Extract the core insight.
6. Return ONLY the JSON object. No other text."""),
            ("human", """ARTICLE TO ANALYZE:
{article_text}

COMPANY CONTEXT:
This article is primarily about {company_name}.

Please analyze and return the JSON object:""")
        ])

    # ... [keep all your existing methods until _enrich_with_llm] ...

    def _enrich_with_llm(self, articles: List[Dict]) -> List[Dict]:
        """Use an LLM to perform all complex classification and enrichment in one shot."""
        enriched_articles = []

        for article in articles:
            # 1. Prepare the text for the LLM
            text_to_analyze = f"TITLE: {article.get('title', '')}\nSUMMARY: {article.get('summary', '')}"
            company_name = article.get('company', '')

            try:
                # 2. Call the LLM using LangChain
                llm_response = self._get_llm_analysis(text_to_analyze, company_name)

                # 3. Parse the LLM's structured response
                llm_analysis = self._parse_llm_response(llm_response)

                # 4. Estimate cost for logging
                cost = 0.0
                if hasattr(llm_response, 'response_metadata'):
                    usage = llm_response.response_metadata.get('token_usage', {})
                    cost = _estimate_cost(self.llm.model_name, usage)
                    if cost > 0:
                        logger.info(f"LLM cost for article analysis: ${cost:.4f}")

                # 5. Create the enriched article
                enriched_article = {
                    **article,
                    **llm_analysis,
                    'industry': 'mobile_phones',
                    'contains_tech_specs': self._contains_tech_specs(article['full_text']),
                    'contains_price_info': self._contains_price_info(article['full_text']),
                    'is_competitive_news': llm_analysis.get('competitive_intelligence_value', False),
                    'potential_threat_level': self._assess_threat_level_based_on_llm(llm_analysis, company_name),
                    'llm_analysis_cost': cost  # Track cost per article
                }
                enriched_articles.append(enriched_article)

            except Exception as e:
                logger.error(f"LLM Analysis failed for article '{article.get('title', '')[:50]}...'. Error: {e}")
                # Fallback: use the original article without LLM enrichment
                enriched_articles.append(article)

        print(f"   LLM successfully enriched {len(enriched_articles)} articles")
        return enriched_articles

    def _get_llm_analysis(self, article_text: str, company_name: str) -> Any:
        """Call the LLM using LangChain's interface."""
        # Create the prompt with variables
        prompt = self.analysis_prompt_template.format_messages(
            article_text=article_text,
            company_name=company_name
        )

        # Invoke the LLM with JSON response format
        response = self.llm.with_structured_output(
            {
                "type": "json_schema",
                "schema": {
                    "type": "object",
                    "properties": {
                        "content_type": {"type": "string"},
                        "sentiment_score": {"type": "number"},
                        "contains_ai_mentions": {"type": "boolean"},
                        "competitive_intelligence_value": {"type": "boolean"},
                        "key_takeaways": {"type": "string"}
                    },
                    "required": ["content_type", "sentiment_score", "contains_ai_mentions",
                               "competitive_intelligence_value", "key_takeaways"]
                }
            }
        ).invoke(prompt)

        return response

    def _parse_llm_response(self, response: Any) -> Dict:
        """Parse the response from LangChain LLM."""
        try:
            # The response should already be a dict due to structured output
            if hasattr(response, 'content'):
                # Handle AIMessage response
                return json.loads(response.content)
            elif isinstance(response, dict):
                # Already a dictionary
                return response
            else:
                # Try to parse as string
                return json.loads(str(response))
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"Failed to parse LLM response: {response}. Error: {e}")
            # Return a default structure if parsing fails
            return {
                "content_type": "general_news",
                "sentiment_score": 0,
                "contains_ai_mentions": False,
                "competitive_intelligence_value": False,
                "key_takeaways": "Analysis failed."
            }

    def _assess_threat_level(self, article: Dict) -> str:
        """Assess the potential threat level of this news"""
        content_type = article.get('content_type', '')
        sentiment = article.get('sentiment_score', 0)
        company = article.get('company', '')
        is_direct = COMPETITOR_PROFILES.get(company, CompetitorProfile("", CompanySize.SMALL, 0, False, "", "", [], [], [])).is_direct_competitor
        # Critical threat: AI features from direct competitors with positive sentiment
        if (is_direct and
            content_type == ContentType.AI_FEATURE.value and
            sentiment >= SentimentScore.POSITIVE.value):
            return ThreatLevel.CRITICAL.value

        # High threat: product launches from direct competitors with positive sentiment
        elif (is_direct and
              content_type == ContentType.PRODUCT_LAUNCH.value and
              sentiment >= SentimentScore.POSITIVE.value):
            return ThreatLevel.HIGH.value

        # Medium threat: any significant development from direct competitors
        elif (is_direct and
              content_type in [ContentType.PRODUCT_LAUNCH.value, ContentType.AI_FEATURE.value,
                              ContentType.PARTNERSHIP.value, ContentType.SOFTWARE_UPDATE.value]):
            return ThreatLevel.MEDIUM.value

        # Low threat: all other news
        else:
            return ThreatLevel.LOW.value

    def _prioritize_articles(self, articles: List[Dict]) -> List[Dict]:
        """Prioritize articles based on threat level and relevance"""
        for article in articles:
            # Add priority based on content type and threat level
            content_type = article.get('content_type', '')
            threat_level = article.get('potential_threat_level', 'low')
            relevance_score = article.get('relevance_score', 0)

            if threat_level == ThreatLevel.CRITICAL.value:
                article['priority'] = "critical"
                article['priority_score'] = 100 + relevance_score
            elif threat_level == ThreatLevel.HIGH.value:
                article['priority'] = "high"
                article['priority_score'] = 80 + relevance_score
            elif threat_level == ThreatLevel.MEDIUM.value:
                article['priority'] = "medium"
                article['priority_score'] = 60 + relevance_score
            else:
                article['priority'] = "low"
                article['priority_score'] = 40 + relevance_score

        # Sort articles by priority score and date
        return sorted(articles, key=lambda x: (x.get('priority_score', 0), x.get('published', '')), reverse=True)

# cleaning_results['cleaned_articles'][0]



"""# Workflow"""

# Cell 7: LangGraph Node Functions
def search_node(state: AgentState) -> AgentState:
    """Node that executes the search agent"""
    print("🔍 Starting Search Agent...")
    try:
        result = search_agent(state)
        print(f"✅ Search completed. Found {len(result['raw_articles'])} articles")
        return result
    except Exception as e:
        print(f"❌ Search failed: {e}")
        return {**state, "error": f"Search failed: {e}", "current_step": "error"}

def cleaning_node(state: AgentState) -> AgentState:
    """Node that executes the cleaning agent"""
    print("🧹 Starting Cleaning Agent...")
    try:
        # Only clean if we have articles
        if not state.get("raw_articles"):
            print("⚠️ No articles to clean")
            return {**state, "cleaned_articles": [], "current_step": "end"}

        result = cleaning_agent(state)
        print(f"✅ Cleaning completed. {len(result['cleaned_articles'])} cleaned articles")
        return result
    except Exception as e:
        print(f"❌ Cleaning failed: {e}")
        return {**state, "error": f"Cleaning failed: {e}", "current_step": "error"}

def error_node(state: AgentState) -> AgentState:
    """Node to handle errors"""
    print(f"🚨 Error occurred: {state.get('error', 'Unknown error')}")
    return state

def success_node(state: AgentState) -> AgentState:
    """Node to handle successful completion"""
    print("🎯 Workflow completed successfully!")

    # Print summary
    cleaned_count = len(state.get("cleaned_articles", []))
    raw_count = len(state.get("raw_articles", []))

    print(f"\n📊 FINAL RESULTS:")
    print(f"   Raw articles found: {raw_count}")
    print(f"   Cleaned articles: {cleaned_count}")
    print(f"   Filtered out: {raw_count - cleaned_count} articles")

    # Count AI-related articles
    ai_articles = sum(1 for article in state.get("cleaned_articles", [])
                     if article.get('contains_ai_mentions', False))
    print(f"   AI-related articles: {ai_articles}")

    # Show company distribution
    company_counts = {}
    for article in state.get("cleaned_articles", []):
        company = article.get('company', 'Unknown')
        company_counts[company] = company_counts.get(company, 0) + 1

    print(f"\n🏢 Articles by Company:")
    for company, count in company_counts.items():
        print(f"   {company}: {count} articles")

    return state

from langgraph.graph import StateGraph, END

def create_workflow() -> StateGraph:
    """Create and configure the agent workflow"""

    # Initialize the graph
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("search", search_node)
    workflow.add_node("clean", cleaning_node)
    workflow.add_node("error", error_node)
    workflow.add_node("success", success_node)

    # Set entry point
    workflow.set_entry_point("search")

    # Define edges (transitions)
    workflow.add_edge("search", "clean")

    # Conditional edges from clean node
    workflow.add_conditional_edges(
        "clean",
        # Decide next node based on state
        lambda state: "error" if state.get("error") else "success",
        {
            "error": "error",
            "success": "success"
        }
    )

    # Final edges to end
    workflow.add_edge("error", END)
    workflow.add_edge("success", END)

    return workflow

# Create the workflow
workflow = create_workflow()
app = workflow.compile()

async def test_workflow():
    """Test the complete agent workflow"""

    # Create initial state
    initial_state: AgentState = {
        "messages": [],
        "competitor_profiles": COMPETITOR_PROFILES,
        "target_regions": ["US", "EU", "KSA"],  # Smaller set for testing
        "raw_articles": [],
        "cleaned_articles": [],
        "current_step": "search",
        "error": "",
        "search_config": {
            "search_timeframe_days": 1,  # Shorter timeframe for testing
        }
    }

    print("🚀 Starting Agent Workflow Test...")
    print("=" * 60)
    print(f"Target companies: {list(COMPETITOR_PROFILES.keys())}")
    print(f"Target regions: {initial_state['target_regions']}")
    print(f"Timeframe: last {initial_state['search_config']['search_timeframe_days']} days")
    print("=" * 60)

    # Run the workflow
    try:
        final_state = await app.ainvoke(initial_state)

        # Display some sample results
        if final_state.get("cleaned_articles"):
            print(f"\n📄 SAMPLE CLEANED ARTICLES:")
            print("=" * 60)
            for i, article in enumerate(final_state["cleaned_articles"][:3]):  # Show first 3
                print(f"{i+1}. {article.get('title', 'No title')[:70]}...")
                print(f"   Company: {article.get('company', 'Unknown')}")
                print(f"   Type: {article.get('content_type', 'Unknown')}")
                print(f"   AI: {article.get('contains_ai_mentions', False)}")
                print(f"   Sentiment: {article.get('sentiment_score', 0)}")
                print()

        return final_state

    except Exception as e:
        print(f"❌ Workflow failed: {e}")
        return initial_state

print("🧪 Testing Agent Workflow with LangGraph")
print("Using mock agents for testing...")

# Run the test - use await directly in Jupyter
result = await test_workflow()

# Final status
if result.get("error"):
    print(f"❌ Workflow ended with error: {result['error']}")
else:
    print("✅ Workflow completed successfully!")
    print(f"📦 Final state keys: {list(result.keys())}")

    # Display detailed results
    print(f"\n📊 DETAILED RESULTS:")
    print("=" * 50)
    for i, article in enumerate(result["cleaned_articles"]):
        print(f"{i+1}. {article['title']}")
        print(f"   → Company: {article['company']}")
        print(f"   → Type: {article['content_type']}")
        print(f"   → AI: {article['contains_ai_mentions']}")
        print(f"   → Threat: {article['potential_threat_level']}")
        print()

